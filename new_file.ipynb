{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "755d9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----Part B-----## \n",
    "##Predictive modelling##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression as SkLR\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a8d0907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.7.1\n",
      "PySpark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "print(\"PySpark version:\", pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ef1df885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: (10500, 17)\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>cost</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>lat</th>\n",
       "      <th>link</th>\n",
       "      <th>lng</th>\n",
       "      <th>phone</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>rating_text</th>\n",
       "      <th>subzone</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>votes</th>\n",
       "      <th>groupon</th>\n",
       "      <th>color</th>\n",
       "      <th>cost_2</th>\n",
       "      <th>cuisine_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>371A Pitt Street, CBD, Sydney</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Hot Pot', 'Korean BBQ', 'BBQ', 'Korean']</td>\n",
       "      <td>-33.876059</td>\n",
       "      <td>https://www.zomato.com/sydney/sydney-madang-cbd</td>\n",
       "      <td>151.207605</td>\n",
       "      <td>02 8318 0406</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>CBD</td>\n",
       "      <td>Sydney Madang</td>\n",
       "      <td>['Casual Dining']</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>False</td>\n",
       "      <td>#e15307</td>\n",
       "      <td>5.243902</td>\n",
       "      <td>#6f706b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shop 7A, 2 Huntley Street, Alexandria, Sydney</td>\n",
       "      <td>80.0</td>\n",
       "      <td>['Cafe', 'Coffee and Tea', 'Salad', 'Poké']</td>\n",
       "      <td>-33.910999</td>\n",
       "      <td>https://www.zomato.com/sydney/the-grounds-of-a...</td>\n",
       "      <td>151.193793</td>\n",
       "      <td>02 9699 2225</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>The Grounds of Alexandria, Alexandria</td>\n",
       "      <td>The Grounds of Alexandria Cafe</td>\n",
       "      <td>['Café']</td>\n",
       "      <td>3236.0</td>\n",
       "      <td>False</td>\n",
       "      <td>#9c3203</td>\n",
       "      <td>7.560976</td>\n",
       "      <td>#6f706b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Level G, The Darling at the Star, 80 Pyrmont ...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>['Japanese']</td>\n",
       "      <td>-33.867971</td>\n",
       "      <td>https://www.zomato.com/sydney/sokyo-pyrmont</td>\n",
       "      <td>151.195210</td>\n",
       "      <td>1800 700 700</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>The Star, Pyrmont</td>\n",
       "      <td>Sokyo</td>\n",
       "      <td>['Fine Dining']</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>False</td>\n",
       "      <td>#7f2704</td>\n",
       "      <td>10.650407</td>\n",
       "      <td>#6f706b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sydney Opera House, Bennelong Point, Circular...</td>\n",
       "      <td>270.0</td>\n",
       "      <td>['Modern Australian']</td>\n",
       "      <td>-33.856784</td>\n",
       "      <td>https://www.zomato.com/sydney/bennelong-restau...</td>\n",
       "      <td>151.215297</td>\n",
       "      <td>02 9240 8000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Circular Quay</td>\n",
       "      <td>Bennelong Restaurant</td>\n",
       "      <td>['Fine Dining', 'Bar']</td>\n",
       "      <td>278.0</td>\n",
       "      <td>False</td>\n",
       "      <td>#7f2704</td>\n",
       "      <td>22.235772</td>\n",
       "      <td>#4186f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20 Campbell Street, Chinatown, Sydney</td>\n",
       "      <td>55.0</td>\n",
       "      <td>['Thai', 'Salad']</td>\n",
       "      <td>-33.879035</td>\n",
       "      <td>https://www.zomato.com/sydney/chat-thai-chinatown</td>\n",
       "      <td>151.206409</td>\n",
       "      <td>02 8317 4811</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Chinatown</td>\n",
       "      <td>Chat Thai</td>\n",
       "      <td>['Casual Dining']</td>\n",
       "      <td>2150.0</td>\n",
       "      <td>False</td>\n",
       "      <td>#a83703</td>\n",
       "      <td>5.630081</td>\n",
       "      <td>#6f706b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address   cost  \\\n",
       "0                      371A Pitt Street, CBD, Sydney   50.0   \n",
       "1      Shop 7A, 2 Huntley Street, Alexandria, Sydney   80.0   \n",
       "2   Level G, The Darling at the Star, 80 Pyrmont ...  120.0   \n",
       "3   Sydney Opera House, Bennelong Point, Circular...  270.0   \n",
       "4              20 Campbell Street, Chinatown, Sydney   55.0   \n",
       "\n",
       "                                       cuisine        lat  \\\n",
       "0   ['Hot Pot', 'Korean BBQ', 'BBQ', 'Korean'] -33.876059   \n",
       "1  ['Cafe', 'Coffee and Tea', 'Salad', 'Poké'] -33.910999   \n",
       "2                                 ['Japanese'] -33.867971   \n",
       "3                        ['Modern Australian'] -33.856784   \n",
       "4                            ['Thai', 'Salad'] -33.879035   \n",
       "\n",
       "                                                link         lng  \\\n",
       "0    https://www.zomato.com/sydney/sydney-madang-cbd  151.207605   \n",
       "1  https://www.zomato.com/sydney/the-grounds-of-a...  151.193793   \n",
       "2        https://www.zomato.com/sydney/sokyo-pyrmont  151.195210   \n",
       "3  https://www.zomato.com/sydney/bennelong-restau...  151.215297   \n",
       "4  https://www.zomato.com/sydney/chat-thai-chinatown  151.206409   \n",
       "\n",
       "          phone  rating_number rating_text  \\\n",
       "0  02 8318 0406            4.0   Very Good   \n",
       "1  02 9699 2225            4.6   Excellent   \n",
       "2  1800 700 700            4.9   Excellent   \n",
       "3  02 9240 8000            4.9   Excellent   \n",
       "4  02 8317 4811            4.5   Excellent   \n",
       "\n",
       "                                 subzone                           title  \\\n",
       "0                                    CBD                   Sydney Madang   \n",
       "1  The Grounds of Alexandria, Alexandria  The Grounds of Alexandria Cafe   \n",
       "2                      The Star, Pyrmont                           Sokyo   \n",
       "3                          Circular Quay            Bennelong Restaurant   \n",
       "4                              Chinatown                       Chat Thai   \n",
       "\n",
       "                     type   votes  groupon    color     cost_2 cuisine_color  \n",
       "0       ['Casual Dining']  1311.0    False  #e15307   5.243902       #6f706b  \n",
       "1                ['Café']  3236.0    False  #9c3203   7.560976       #6f706b  \n",
       "2         ['Fine Dining']  1227.0    False  #7f2704  10.650407       #6f706b  \n",
       "3  ['Fine Dining', 'Bar']   278.0    False  #7f2704  22.235772       #4186f4  \n",
       "4       ['Casual Dining']  2150.0    False  #a83703   5.630081       #6f706b  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.read_csv(\"C:/Users/aravi/Downloads/zomato_df_final_data.csv\")\n",
    "print(f\"Dataset dimensions: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d7562b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['address', 'cost', 'cuisine', 'lat', 'link', 'lng', 'phone', 'rating_number', 'rating_text', 'subzone', 'title', 'type', 'votes', 'groupon', 'color', 'cost_2', 'cuisine_color']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e6231d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n",
      "address           object\n",
      "cost             float64\n",
      "cuisine           object\n",
      "lat              float64\n",
      "link              object\n",
      "lng              float64\n",
      "phone             object\n",
      "rating_number    float64\n",
      "rating_text       object\n",
      "subzone           object\n",
      "title             object\n",
      "type              object\n",
      "votes            float64\n",
      "groupon             bool\n",
      "color             object\n",
      "cost_2           float64\n",
      "cuisine_color     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "31df6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "10a0eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "address             0\n",
      "cost              346\n",
      "cuisine             0\n",
      "lat               192\n",
      "link                0\n",
      "lng               192\n",
      "phone               0\n",
      "rating_number    3316\n",
      "rating_text      3316\n",
      "subzone             0\n",
      "title               0\n",
      "type               48\n",
      "votes            3316\n",
      "groupon             0\n",
      "color               0\n",
      "cost_2            346\n",
      "cuisine_color       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##1.Handling missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4f0053b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are going to handle the missing the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4bfea1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we impute numeric columns with -1 and categorical values with 'unknown'\n",
    "for col in df.columns:\n",
    "     # categorical values\n",
    "    if df[col].dtype == \"object\":  \n",
    "        df[col] = df[col].fillna(\"Unknown\")\n",
    "    else:            \n",
    "        # numeric values               \n",
    "        df[col] = df[col].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "85661f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers in cost: 361\n"
     ]
    }
   ],
   "source": [
    "#Now we are dropping values by detecting outliers using the IQR method\n",
    "Q1 = df['cost'].quantile(0.25)\n",
    "Q3 = df['cost'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = df[(df['cost'] < (Q1 - 1.5 * IQR)) | (df['cost'] > (Q3 + 1.5 * IQR))]\n",
    "print(f\"Number of outliers in cost: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "64ca7517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing outliers: (10139, 17)\n"
     ]
    }
   ],
   "source": [
    "#since we have these many outliers we need to drop them\n",
    "df = df[~((df['cost'] < (Q1 - 1.5 * IQR)) | (df['cost'] > (Q3 + 1.5 * IQR)))]\n",
    "print(\"Dataset after removing outliers:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fcedb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encoding categorical features##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f03ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e64dc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f8a04da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Average': np.int64(0), 'Excellent': np.int64(1), 'Good': np.int64(2), 'Poor': np.int64(3), 'Unknown': np.int64(4), 'Very Good': np.int64(5)}\n"
     ]
    }
   ],
   "source": [
    "df['rating_text_encoded'] = le.fit_transform(df['rating_text'])\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7d9c67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating useful features##\n",
    "##some of the useful features includes, cuisine diversity,cost,votes,ratings in number and texts, subzone etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "645f38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysing the number of cuisines, the restaurant serves\n",
    "df['cuisine_count'] = df['cuisine'].apply(lambda x: len(str(x).split(',')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "df1cbe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        4\n",
      "1        4\n",
      "4        2\n",
      "7        1\n",
      "9        2\n",
      "        ..\n",
      "10495    1\n",
      "10496    1\n",
      "10497    1\n",
      "10498    1\n",
      "10499    1\n",
      "Name: cuisine_count, Length: 10139, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['cuisine_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "03e48102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "##finding the number of people voted on the basis of voting\n",
    "df['rating_strength'] = df['rating_number'] * np.log1p(df['votes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a51128ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        28.717232\n",
      "1        37.179050\n",
      "4        34.531597\n",
      "7        33.368528\n",
      "9        33.831289\n",
      "           ...    \n",
      "10495          inf\n",
      "10496          inf\n",
      "10497          inf\n",
      "10498          inf\n",
      "10499          inf\n",
      "Name: rating_strength, Length: 10139, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['rating_strength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4f0ba038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding out the restaurants which are popular and not. \n",
    "df['is_popular'] = (df['votes'] > df['votes'].median()).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6ab17cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        1\n",
      "4        1\n",
      "7        1\n",
      "9        1\n",
      "        ..\n",
      "10495    0\n",
      "10496    0\n",
      "10497    0\n",
      "10498    0\n",
      "10499    0\n",
      "Name: is_popular, Length: 10139, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['is_popular'])\n",
    "#if it shows 1 it is popular and if it show 0 its not popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ebf80114",
   "metadata": {},
   "outputs": [],
   "source": [
    "##finding the cost bins\n",
    "bins = [0, 50, 100, 200, 400, 800, 1600, float('inf')]\n",
    "labels = ['Very Low', 'Low', 'Mid-Low', 'Mid', 'Mid-High', 'High', 'Luxury']\n",
    "df['cost_bin'] = pd.cut(df['cost'], bins=bins, labels=labels, include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fd9668d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Very Low\n",
       "1             Low\n",
       "4             Low\n",
       "7             Low\n",
       "9             Low\n",
       "           ...   \n",
       "10495    Very Low\n",
       "10496    Very Low\n",
       "10497    Very Low\n",
       "10498    Very Low\n",
       "10499         Low\n",
       "Name: cost_bin, Length: 10139, dtype: category\n",
       "Categories (7, object): ['Very Low' < 'Low' < 'Mid-Low' < 'Mid' < 'Mid-High' < 'High' < 'Luxury']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cost_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7ab0367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##----2.Regression Models------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ef5bf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Model A-- Linear Regression---#\n",
    "\n",
    "##Here we are predicting rating_number(target variable) using Scikit-learn\n",
    "##For this we are using features like cuisine_count, cost, votes etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1f768587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dummy variables for rating_text (if not already created)\n",
    "if 'rating_text' in df.columns:\n",
    "    dummies = pd.get_dummies(df['rating_text'], prefix='rating_text')\n",
    "    df = pd.concat([df, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "14ee8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features chosen\n",
    "feature_cols = ['cost','votes','cuisine_count',\n",
    "                'rating_text_Excellent','rating_text_Good',\n",
    "                'rating_text_Poor','rating_text_Very Good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9772623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols]\n",
    "y = df['rating_number']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "39a8b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = df['rating_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "47f5a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train-test split\n",
    "# -------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7a8df9bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 3: Train Linear Regression\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m model.fit(X_train, y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\__init__.py:115\u001b[39m, in \u001b[36mkeyword_only.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m forces keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m % func.\u001b[34m__name__\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m._input_kwargs = kwargs\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\regression.py:329\u001b[39m, in \u001b[36mLinearRegression.__init__\u001b[39m\u001b[34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[33;03m__init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[33;03m         maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m         standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2, \\\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m         loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28msuper\u001b[39m(LinearRegression, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m \u001b[38;5;28mself\u001b[39m._java_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.ml.regression.LinearRegression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muid\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m._input_kwargs\n\u001b[32m    333\u001b[39m \u001b[38;5;28mself\u001b[39m.setParams(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\util.py:313\u001b[39m, in \u001b[36mtry_remote_return_java_class.<locals>.wrapped\u001b[39m\u001b[34m(java_class, *args)\u001b[39m\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m java_class\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:98\u001b[39m, in \u001b[36mJavaWrapper._new_java_obj\u001b[39m\u001b[34m(java_class, *args)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[32m     97\u001b[39m sc = SparkContext._active_spark_context\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    100\u001b[39m java_obj = _jvm()\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 3: Train Linear Regression\n",
    "# -------------------------\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8291ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions\n",
    "y_prediction_modelA = modelA.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_prediction_modelA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77949604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Mean squared error(MSE)\n",
    "mse_modelA = mean_squared_error(y_test, y_prediction_modelA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE value for Linear Regression using Scikit-learn is: ',mse_modelA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model B- Gradient Descent Regression - implementing linear regression with gradient descent##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c949be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are using the features implemented for model A as well#\n",
    "\n",
    "X = df[['cost', 'votes', 'cuisine_count', \n",
    "        'rating_text_Excellent', 'rating_text_Good', \n",
    "        'rating_text_Poor', 'rating_text_Very Good']].values\n",
    "y = df['rating_number'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    " # implementing 80/20 split for train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32bd2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##implementing feature scaling needed for gradient descent\n",
    "##for this we are doing scaler for generalising features needed for gradient descent.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c262a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_b = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_test_b = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff43402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying gradient descent \n",
    "\n",
    "def gradient_descent(X, y, lr=0.01, n_iter=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)  # initialize weights\n",
    "    for _ in range(n_iter):\n",
    "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
    "        theta -= lr * gradients\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b252cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "theta = gradient_descent(X_train_b, y_train, lr=0.01, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a79e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions\n",
    "\n",
    "y_prediction_modelB = X_test_b.dot(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99eca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcuating Mean Squared Value(MSE)\n",
    "mse_modelB = mean_squared_error(y_test, y_prediction_modelB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE value for Gradient descent regression: ',mse_modelB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f84973",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. CLASSIFICATION MODELS ###\n",
    "# here we dont have rating _text column. Here we are are going to have 2 classes.\n",
    "#class 1 : Poor + Average\n",
    "#class 2 : Good + Very Good + Excellent\n",
    "\n",
    "#we set class1 as the default class here\n",
    "df['rating_binary'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set those having rating as goog or very good or excellent as class 2\n",
    "df.loc[(df['rating_text_Good'] == 1) | \n",
    "       (df['rating_text_Very Good'] == 1) | \n",
    "       (df['rating_text_Excellent'] == 1), 'rating_binary'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['rating_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e486c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying logistic regression keeping rating_binary as target variable\n",
    "\n",
    "X = df[['cost', 'votes', 'cuisine_count']]   \n",
    "y = df['rating_binary']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are doing train-test split for (80/20) here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing training for logistic regression\n",
    "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
    "logistic_regression_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions\n",
    "y_prediction = logistic_regression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85eade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation with confusion matrix which shows how many restaurants were classified properly\n",
    "confusion_matrix = confusion_matrix(y_test, y_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd036a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix: \",confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation using precision which depicts the to what extent the predicition of classification was proper\n",
    "precision = precision_score(y_test, y_prediction, pos_label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PRECISION : \",precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12009dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for recall which explains how much the model correctly identified.\n",
    "recall = recall_score(y_test, y_prediction, pos_label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5872629",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95469fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for F1 which explains how balanced, the values of precision and recall are\n",
    "f1_Score = f1_score(y_test, y_pred, pos_label=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d803f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score : \",f1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec57357",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAININIG 3 MORE MODELS##\n",
    "\n",
    "#Initiailizing a variable to store the results of each model and show it as a table\n",
    "\n",
    "results_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882aebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Random Forest model\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "y_pred_random_forest = random_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e078b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_models['Random Forest'] = {\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred_random_forest).tolist(),\n",
    "    'Precision': round(precision_score(y_test, y_pred_random_forest, pos_label=2), 3),\n",
    "    'Recall': round(recall_score(y_test, y_pred_random_forest, pos_label=2), 3),\n",
    "    'F1': round(f1_score(y_test, y_pred_random_forest, pos_label=2), 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_models['Random Forest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Gradient Boosted trees##\n",
    "gradient_boost_model = GradientBoostingClassifier(random_state=42)\n",
    "gradient_boost_model.fit(X_train, y_train)\n",
    "y_pred_gradient_boost = gradient_boost_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_models['Gradient Boosted Trees'] = {\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred_gradient_boost).tolist(),\n",
    "    'Precision': round(precision_score(y_test, y_pred_gradient_boost, pos_label=2), 3),\n",
    "    'Recall': round(recall_score(y_test, y_pred_gradient_boost, pos_label=2), 3),\n",
    "    'F1': round(f1_score(y_test, y_pred_gradient_boost, pos_label=2), 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_models['Gradient Boosted Trees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVM Model ###\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm_model= svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_models['SVM'] = {\n",
    "    'Confusion Matrix': confusion_matrix(y_test, y_pred_svm_model).tolist(),\n",
    "    'Precision': round(precision_score(y_test, y_pred_svm_model, pos_label=2), 3),\n",
    "    'Recall': round(recall_score(y_test, y_pred_svm_model, pos_label=2), 3),\n",
    "    'F1': round(f1_score(y_test, y_pred_svm_model, pos_label=2), 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_models['SVM'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c278d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting the data we got into a table form for comparison\n",
    "\n",
    "results_data_frame = pd.DataFrame(results_models).T\n",
    "print(results_data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed08eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session for this notebook\n",
    "spark = SparkSession.builder.appName(\"Zomato-MLlib\").getOrCreate()\n",
    "print(\"Spark started ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we are bringing the data to spark\n",
    "df_spark = spark.createDataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62d052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
